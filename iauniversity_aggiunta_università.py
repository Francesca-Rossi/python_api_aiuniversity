# -*- coding: utf-8 -*-
"""IAuniversity_aggiunta_università.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Francesca-Rossi/ITA_university_advisor/blob/main/IAuniversity_backup_11_06_21.ipynb

<center><h1>PROGETTO DI BIG DATA E BUSINESS INTELLIGENCE:</h1>

![](https://drive.google.com/uc?export=view&id=1akQN6cxOyAOmI5QRIw1VVnQUUIU8WFrT)
<p><i>An italian university advisor</i></p>
<p> A cura di Francesca Rossi e Martina Dominici </p>
</center>

<h4><b>1) Obiettivo della ricerca </b></h4>
<p>Lo scopo di questo progetto è creare un consigliatore di università triennali/magistrali a ciclo unico sfruttando la potenza del Machine Learning,aiutando nella scelta gli studenti delle scuole superiori.</p>

<h4><b>2) Stato dell'arte </b></h4>
<p>Questo argomento non presentava ricerche simili o uguali fatte in precedenza.Questo ha comportato che non fosse presente un dataset,problema al quale abbiamo ovviato autoproducendolo.</p>

<h4><b>3) Strumenti utilizzati </b></h4>
<table>
<tr>
  <th>Strumento</th>
  <th>Descrizione</th>
  <th>Utilizzo</th>
</tr>
<tr>
<td>Google Apps Script</td>
<td>Apps Script è una piattaforma di scripting sviluppata da Google per lo sviluppo di applicazioni leggere nella piattaforma Google Workspace.Come linguaggio viene utilizzato typescript </td>
<td>Creazione del questionario</td>
</tr>
<tr>
<td>Google Sheets</td>
<td>Google Sheets è un programma per fogli di calcolo incluso come parte della suite di editor di documenti Google gratuita basata sul Web offerta da Google</td> 
<td>Memorizzazione dei dati raccolti</td>
</tr>
<tr>
<td>Google Colab</td>
<td>Google Colaboratory (noto anche come Colab ) è un ambiente di notebook Jupyter gratuito che viene eseguito nel cloud e memorizza i suoi notebook su Google Drive</td>
<td>Creazione del progetto di machine learning</td>
</tr>
<tr>
<td>Python</td>
<td>Python è un linguaggio di programmazione di "alto livello", orientato a oggetti, adatto, tra gli altri usi, a sviluppare applicazioni distribuite, scripting, computazione numerica e system testing.</td>
<td>Creazione del progetto di machine learning</td>
</tr>
<tr>
<td>Pandas</td>
<td>Pandas è una libreria software scritta per il linguaggio di programmazione Python per la manipolazione e l'analisi dei dati. In particolare, offre strutture dati e operazioni per manipolare tabelle numeriche e serie temporali</td>
<td>Costruzione e manipolazione vari dataset</td>
</tr>
<tr>
<td>Numpy</td>
<td>NumPy è una libreria open source per il linguaggio di programmazione Python, che aggiunge supporto a grandi matrici e array multidimensionali insieme a una vasta collezione di funzioni matematiche di alto livello per poter operare efficientemente su queste strutture dati</td>
<td>Pulizia e preprocessing dei dati</td>
</tr>
<tr>
<td>Sklearn</td>
<td>Scikit-learn (ex scikits.learn) è una libreria open source di apprendimento automatico per il linguaggio di programmazione Python. Contiene algoritmi di classificazione, regressione e clustering (raggruppamento) e macchine a vettori di supporto, regressione logistica, classificatore bayesiano e k-mean </td>
<td>Addestramento e valutazione degli algoritmi</td>
</tr>
<tr>
<td>Nltk</td>
<td>Il Natural Language Toolkit (toolkit per il linguaggio naturale), più comunemente conosciuto come NLTK, è una suite di librerie e programmi per l'analisi simbolica e statistica nel campo dell'elaborazione del linguaggio naturale scritta in linguaggio Python</td>
<td>Pulizia e preprocessing dei dati</td>
</tr>
<tr>
<td>MatplotLib</td>
<td>Matplotlib è una libreria per la creazione di grafici per il linguaggio di programmazione Python e la libreria matematica NumPy</td>
<td>Visualizzazione dei risultati</td>
</tr>
<tr>
<td>Seaborn</td>
<td> fornisce un'API in aggiunta a Matplotlib che offre scelte per lo stile di stampa e le impostazioni predefinite del colore, definisce semplici funzioni di alto livello per i comuni tipi di grafici statistici e si integra con le funzionalità fornite da Pandas</td>
<td>Visualizzazione dei risultati</td>
</tr>
</table>

#<p>IMPORTAZIONI LIBRERIE</p>
"""

import gspread 
import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import plot_confusion_matrix
from sklearn.cluster import KMeans
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from collections import Counter
from numpy import where
from numpy import mean
from numpy import std
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE
import os
from oauth2client.service_account import ServiceAccountCredentials
#from google.colab import drive
from nltk.corpus import stopwords
from nltk.stem.snowball import ItalianStemmer
import json as js
import seaborn as sns

"""#<p>FUNZIONI PRINCIPALI</p>"""
def principal():
  stop_ita = stopwords.words('italian')
  stop_en = stopwords.words('english')
  stemmer = ItalianStemmer()


  #CLEANING DATA
  def lang_stop_words(feature, lang):
    if (lang == "IT"):
      feature=feature.apply(lambda x: [item for item in x if item not in stop_ita])
    elif (lang == "EN"):
      feature=feature.apply(lambda x: [item for item in x if item not in stop_en])
    else:
      feature=feature.apply(lambda x: [item for item in x if item not in stop_ita])
      feature=feature.apply(lambda x: [item for item in x if item not in stop_en])
    return feature

  def data_cleaning(feature, regex_list):
    feature=feature.apply(lambda x: x.lower()) #lowercase
    feature=feature.str.strip() #elimination of white spaces at the beginning and at the end
    for regex in regex_list:
      feature=feature.str.replace(regex, ' ')
    for char in string.punctuation:
      feature=feature.str.replace(char, ' ')
    return feature

  #PREPROCESSING TEXT
  def preprocessing_text(dataframe, feature, key, lang):
    feature=feature.str.strip() #elimination of white spaces at the beginning and at the end
    feature =dataframe.apply(lambda row: nltk.word_tokenize(row[key]), axis=1)
    feature =lang_stop_words(feature, lang)
    feature=feature.apply(lambda x: [stemmer.stem(y) for y in x]) # stem every word
    feature=feature.apply(' '.join)
    return feature

  #FEATURE ENGINEERING TEXT
  def tf_vector(vectorize, feature):
    tf_matrix = vectorize.fit_transform(feature)
    tf_matrix = tf_matrix.toarray()
    vocab = vectorize.get_feature_names()
    return pd.DataFrame(np.round(tf_matrix, 2), columns=vocab)

  def bag_of_words(vectorize,feature):
    vectorize.fit(feature)
    bag_array= vectorize.transform(feature).toarray()
    vocab = vectorize.get_feature_names()
    return pd.DataFrame(bag_array, columns=vocab)

  #FEATURE ENGINEERING CATEGORICAL DATA

  def one_hot_encoding(feature, key, genre_labels, encoder):
    genre_mappings = {index: label for index, label in enumerate(encoder.classes_)}
    feature=genre_labels
    print('encoding', key, ':',  genre_mappings)
    return feature

  #TRAINING DATASET
  def fit_dataset(model, X_train, y_train, X_test):
    model.fit(X_train, y_train) # training the model on the train dataset
    predictions= model.predict(X_test) # predicting the output on the test dataset
    return predictions

  def score_dataset(model, X_train, y_train, X_test, y_test):
    score_dict={}
    score_dict['score_train_set']=model.score(X_train, y_train)
    score_dict['score_test_set']=model.score(X_test, y_test)
    score_dict['lengh_train_set'] = round(len(X_train))
    score_dict['lengh_test_set'] = round(len(X_test))
    return score_dict

  def wrong_classification(X_test, predictions, y_test):
    wrong_class=0
    for i in range(len(X_test)):
      elem = X_test[i]
      prediction = predictions[i]
      label = y_test[i]
      if prediction != label: #output for debug
          wrong_class += 1
          print(i, 'has been classified as ', prediction, 'and should be ', label)
    print("-----------------------------------")
    return wrong_class

  #GRAPHS
  # missing value
  def missing_value_colors(values, tot):
    clrs=[]
    for x in values:
      x_perc= (x/tot)*100
      if (x_perc<=75):
        clrs.append('g') #green when missing value < 75%
      elif (x_perc>75) and (x_perc<=85):
        clrs.append('y')
      else:
        clrs.append('r')
    return clrs

  def missing_value_graphs(dataframe_missing_value, original_df, title):
    #variable
    tot_values=original_df.shape[0]
    print(tot_values)
    values=dataframe_missing_value.values
    row_index=dataframe_missing_value.index
    start=0
    for i in range(2, 0, -1):
      end=int(len(row_index)/i)
      title_str=f'{title} valori mancanti  da {row_index[start]} a {row_index[end-1]}'
      #graphs
      fig, ax = plt.subplots(figsize =(10, 12))
      colors=missing_value_colors(values, tot_values)
      ax.barh(row_index[start:end], values[start:end], color=colors[start:end])   # Horizontal Bar Plot
      ax.grid(b = True, color ='grey', linestyle ='-.', linewidth = 0.5, alpha = 0.2)   # Add x, y gridlines
      for i in ax.patches:
          plt.text(i.get_width()+0.2, i.get_y()+0.5,str(round(((i.get_width()/tot_values)*100), 2))+'%',
                  fontsize = 10, fontweight ='bold',color ='grey')   # Add annotation to bars
      ax.invert_yaxis()   # Show top values
      #labeling
      plt.xlabel("Valori mancanti")
      plt.ylabel("Caratteristiche")
      plt.title(title_str)
      plt.show() #show the graphs
      start=end

  """#<h4>4) RECUPERO DEI DATI E COSTRUZIONE DATASET INIZIALE</h4>
  <p>I dati per la ricerca sono stati raccolti utilizzando un <b>form autoprodotto</b>, successivamente memorizzati all'interno di un <b>file google sheets</b>.</p>
  
  ## <h5><b>4.1) Connessione con il foglio di google</b></h5>
  """

  SCOPES = ["https://spreadsheets.google.com/feeds",
                    "https://www.googleapis.com/auth/spreadsheets",
                    "https://www.googleapis.com/auth/drive",
                    "https://www.googleapis.com/auth/drive"]
  drive.mount('/content/drive')
  os.chdir("/content/drive/My Drive/Colab Notebooks/big data proj")
  cred = ServiceAccountCredentials.from_json_keyfile_name("/content/drive/My Drive/Colab Notebooks/big data proj/GoogleSheetCredentials.json", SCOPES)
  gclient = gspread.authorize(cred)
  sheet = gclient.open_by_url("https://docs.google.com/spreadsheets/d/1scMc0UlNC1pdZNow5k3pLhofcW6j4ekXyxYlNiBcueI/edit?usp=sharing")



  """##<h5><b> 4.2) Creazione dei dataset iniziale</b></h5>"""

  students_data = sheet.worksheet('Laureando').get_all_records()
  graduate_data = sheet.worksheet('Laureato').get_all_records()

  df_students = pd.DataFrame(students_data)
  df_graduate = pd.DataFrame(graduate_data)
  print("students shape:", df_students.shape )
  print("graduates shape:", df_graduate.shape )

  """##<h5><b> 4.3) Rimozione features in eccesso</b> </h5>
  <p> Abbiamo rimosso le features relative ad un altro progetto evitantando cosi di oberare la memoria con dati non significativi </p>
  """

  df_students = df_students.drop(df_students.loc[:, 'degree_course_class':'decision_choice_class'].columns, axis=1)
  df_graduate = df_graduate.drop(df_graduate.loc[:, 'degree_course_class':'decision_choice_class'].columns, axis=1)
  print("students shape:", df_students.shape )
  print("graduates shape:", df_graduate.shape )

  """#<h4>5) PREPROCESSING E DATA CLEANING</h4>
  
  ##<h5><b> 5.1) Analisi dei valori mancanti </b></h5>
  <p> Avendo raccolto i dati di prima persona, creando un dataset autoprodotto, abbiamo avuto la necessità di visionare e gestire quanti e quali fossero i dati mancanti.</p>
  
  ###<h5>5.1.1) Sostituzione celle vuote con valore 'Nan'</h6>
  """

  df_students=df_students.replace('',np.NaN)
  df_graduate=df_graduate.replace('',np.NaN)

  """###<h5> 5.1.2) Somma valori mancanti per ogni features</h6>"""

  df_missing_value_students=df_students.isnull().sum()
  df_missing_value_graduate=df_graduate.isnull().sum()

  """### <h5> 5.1.3) DATASET STUDENTI: grafico valori mancanti</h6>
  
  """

  missing_value_graphs(df_missing_value_students, df_students, 'Studenti')

  """### <h5> 5.1.4) LAUREATI: grafico valori mancanti </h6>"""

  missing_value_graphs(df_missing_value_graduate, df_graduate, 'Laureati')


  """##<h5><b> 5.2) Recupero dati triennali e magistrali a ciclo unico</b><h5>
  
  <p>Dopo un'accurata analisi dei valori abbiamo riscontrato un numero molto elevato di valori mancanti relativi alle lauree specialistiche che non avrebbe permesso una predizione finale ottimale. Abbiamo perciò optato per continuare la nostra ricerca con le lauree di primo livello.</p>
  """

  query_first_degree='study_type == "Triennale" | study_type == "Magistrale_unico"'
  df_students_first_degree = df_students.query(query_first_degree)
  df_graduate_first_degree = df_graduate.query(query_first_degree)
  print('students first degree:',df_students_first_degree.shape)
  print('graduates first degree:',df_graduate_first_degree.shape)

  """### <h5>5.2.1) Analisi valori mancanti specifica per le lauree di primo livello</h5>"""

  df_missing_value_first_degree_students=df_students_first_degree.isnull().sum()
  df_missing_value_first_degree_graduate=df_graduate_first_degree.isnull().sum()

  """####<h6> 5.2.1.1) STUDENTI: grafico valori mancanti</h6>"""

  missing_value_graphs(df_missing_value_first_degree_students, df_students_first_degree, 'Studenti laurea primo livello')

  """#### <h6> 5.2.1.2)LAUREATI: grafico valori mancanti </h6>"""

  missing_value_graphs(df_missing_value_first_degree_graduate, df_graduate_first_degree, 'Laureati laurea primo livello')

  """##<h5><b>5.3) Selezione delle features</b></h5>
  <p>Dopo una seconda analisi specifica dei valori mancanti relativi alle lauree di primo livello, abbiamo selezionato le features più consone per il nostro obiettivo finale e con un numero di valori mancanti ridotto, quali:</p>
  <ul>
  <li>Main Subject</li>
  <li>Favorite Subject</li>
  <li>Main Subject</li>
  <li>Dream job</li>
  <li>Hobby</li>
  <li>High school</li>
  <li>Decision Choice</li>
  <li>Expectations </li>
  <li>Choice Related Studies</li>
  </ul>
  <p>Abbiamo inoltre selezionato la feature <code> other high school </code>,nel caso fosse possibile raccogliere ulteriori dati. Questo campo sarà gestito  assieme al campo <code> high school </code> nei prossimi passaggi</p>.
  """

  features_first_degree=['degree_course','other_high_school','high_school','main_subject', 'favorite_subject', 'dream_job', 'hobby', 'decision_choice' , 'expectations', 'choice_related_studies' ]

  """###<h5> 5.3.1) Unione del dataset degli studenti con quello dei laureati per le features selezionate</h5>
  <p>Poichè la nostra ricerca si concentrava sulle lauree di primo livello,   abbiamo unito i dataset concernenti laureandi e laureati che rientravano nel target, analizzando i dati ottenuti per effettuare una predizione più completa ed accurata</p>
  """

  all_features_dict={}
  for feature in features_first_degree:
    all_features=df_students_first_degree[feature]
    all_features=all_features.append(df_graduate_first_degree[feature], ignore_index=True, verify_integrity=True)
    all_features_dict[feature]=all_features

  df_all_features_first_degree=pd.DataFrame(all_features_dict)
  print('original sample first degree:',df_all_features_first_degree.shape[0] )
  df_all_features_first_degree.head()

  """##<h5><b> 5.5) Features cleaning</b></h5>

  ###<h5> 5.5.1) Eliminazione valori nulli </h5>
  """

  #count null values
  df_all_features_first_degree.isnull().sum()

  for key in all_features_dict:
    if key!='main_subject' and key!='other_high_school':
      df_all_features_first_degree = df_all_features_first_degree[df_all_features_first_degree[key].notna()]
  print('sample first degree after delete null value:',df_all_features_first_degree.shape[0])

  """### <h5>5.5.2) Gestione valori mancanti del <code>main_subject</code><h5>
  <p> Per effettuare la gestione dei valori mancanti delle materie principali nel campo <code>main_subject</code> abbiamo ipotizzato che ciascuna categoria di scuole superiori abbia le stesse materie principali. </p>
  <p> Conoscendo la scuola superiore, abbiamo perciò realizzato un algoritmo il cui scopo è replicare le materie mancanti data la scuola superiore. <br> Il funzionamento è il seguente: l'algoritmo cerca la prima riga non nulla contenete le materie principali relative a quella scuola superiore e le replica nel caso in cui la stessa scuola superiore abbia il campo della materia principale vuoto</p>
  
  """

  school_whitout_subject= df_all_features_first_degree['high_school'].loc[df_all_features_first_degree['main_subject'].isnull()]

  full_subject=df_all_features_first_degree['main_subject']
  high_school=np.array(df_all_features_first_degree['high_school'])
  for x  in school_whitout_subject:
    subject=np.array(df_all_features_first_degree['main_subject'].loc[df_all_features_first_degree['high_school'].str.contains(x,na=False)][0:1])
    for i in range(0, len(high_school)):
      if (high_school[i]==x):
          full_subject[i]=np.array2string(subject)
  df_all_features_first_degree['full_subject']=full_subject

  #replace missing value in main subject whit the new value in full subject
  df_all_features_first_degree['main_subject']=df_all_features_first_degree['main_subject'].combine_first(df_all_features_first_degree['full_subject'])
  df_all_features_first_degree[['main_subject', 'full_subject']].head(50)

  df_all_features_first_degree=df_all_features_first_degree.drop(['full_subject'], axis=1) #eliminate the support column

  """Eliminazione eventuali valori nulli rimasti ed aggiunta al dizionario con il resto delle colonne pulite"""

  df_all_features_first_degree = df_all_features_first_degree[df_all_features_first_degree['main_subject'].notna()] #delete null values
  print('sample first degree after delete null value:',df_all_features_first_degree.shape[0])

  original_df_whitout_nan_value=df_all_features_first_degree.copy()

  """###<h5>5.5.3) Pulizia del target (<code>degree_course</code>)</h5>
  <p>Poichè la nostra variabile target è stata immessa dall'utente come testo libero, abbiamo dovuto ripulirla per renderla idonea alla feature engineering.</p>
  <p>Oltre alle normali procedure di pulizia del testo (stemming, eliminazione stopwords...) è stato necessario: <ul> 
  <li>sostituire gli acronimi (che abbiamo trovato analizzando i dati) con il relativo corso di laurea</li>
  <li>eliminare parole speciali e fuorvianti quali "curriculum", "indirizzo", "L-22".....</li>
  <li>Fare un ulteriore pulizia manuale delle lauree scritte in maniera scorretta (invertito il nome ecc...)</li>
  </ul></p>
  <p>Per quest'ultima pulizia più approfondita è stata per noi cruciale la conoscienza del dominio</p>
  """

  #acronimi
  slang_degree_course={"ctf":"chimica e tecnologie farmaceutiche",
         "clemi": "economia e marketing internazionale",
         "sztpa": "scienze zootecniche e tecnologie delle produzioni animali",
         "smid": "statistica matematica e trattamento informatico dati",
         "clei": "economia internazionale",
         "clem": "economia e marketing",
         "cle": "economia",
         "dams": "discipline delle arti della musica e dello spettacolo",
         "ctc": "chimica e tecnologie chimiche",
         "pmts": "planning management of tourism systems",
         "eifi":  "lingue moderne comunicazione cooperazione internazionale",
         "lms":  "letteratura musica spettacolo"
          }
  regex_degree_course=[r'\sl.*[0-9]+', r'\scurriculum\s.*', r'indirizzo.*', r'[(].*[)]*', r'&', r'\s\s+', r'corso di laurea', r'ciclo unico', r'’']

  #delete null value
  df_all_features_first_degree=df_all_features_first_degree[df_all_features_first_degree['degree_course'].notna()]

  df_all_features_first_degree['degree_course']=data_cleaning(df_all_features_first_degree['degree_course'], regex_degree_course)
  #replace slang
  for key in slang_degree_course.keys():
    df_all_features_first_degree.loc[df_all_features_first_degree['degree_course'].str.contains(key), 'degree_course'] = slang_degree_course[key]

  #another cleaning
  df_all_features_first_degree.loc[df_all_features_first_degree['degree_course'].str.contains('medicina') & ~df_all_features_first_degree['degree_course'].str.contains('veterinaria'), 'degree_course'] = 'medicina e chirurgia'
  df_all_features_first_degree.loc[df_all_features_first_degree['degree_course'].str.contains('informatica') & df_all_features_first_degree['degree_course'].str.contains('elettronica') & df_all_features_first_degree['degree_course'].str.contains('telecomunicazioni') , 'degree_course'] = 'ingegneria informatica elettronica e telecomunicazioni'
  df_all_features_first_degree.loc[df_all_features_first_degree['degree_course'].str.contains('comunicazione e media per le industrie creative'), 'degree_course'] = 'comunicazione e media contemporanei per le industrie creative'
  df_all_features_first_degree.loc[df_all_features_first_degree['degree_course'].str.contains('elettronica') & ~df_all_features_first_degree['degree_course'].str.contains('ingegneria' )& ~df_all_features_first_degree['degree_course'].str.contains('ing'), 'degree_course'] = 'ingegneria elettronica'
  df_all_features_first_degree.loc[df_all_features_first_degree['degree_course'].str.contains('interfacce e tecnologie della comunicazion'), 'degree_course'] = 'interfacce uomo macchina e tecnologie della comunicazione'
  df_all_features_first_degree.loc[df_all_features_first_degree['degree_course'].str.contains('letteratura musica e spettacolo'), 'degree_course'] = 'letteratura musica e spettacolo'
  df_all_features_first_degree['degree_course']=df_all_features_first_degree['degree_course'].str.replace(r'(^ingegneria\s*)|(^ingegneria)', 'ing ')
  df_all_features_first_degree['degree_course']=df_all_features_first_degree['degree_course'].str.replace('ingeneria ', 'ing ')
  df_all_features_first_degree['degree_course']

  df_all_features_first_degree['degree_course']=preprocessing_text(df_all_features_first_degree, df_all_features_first_degree['degree_course'], 'degree_course', 'IT-EN')

  #DEBUG: print the unique degree_course after the cleaning
  unique_degree_course=np.unique(df_all_features_first_degree['degree_course'])
  print(len(unique_degree_course))
  print(unique_degree_course)

  """###<h5>5.5.5) Unione tra <code>high_school</code> e <code>other_high_school</code></h5>
  <p> Questa unione è stata pensata nel caso in cui un studente frequentasse una scuola superiore particolare non facente parte dell'elenco che abbiamo fornito agli studenti</p>
  
  
  """

  #need to merge 2 columns
  df_all_features_first_degree['high_school']=df_all_features_first_degree['high_school'].replace('altro', np.NaN )

  #merge columns
  df_all_features_first_degree['high_school']=df_all_features_first_degree['high_school'].combine_first(df_all_features_first_degree['other_high_school'])

  df_all_features_first_degree['high_school']=df_all_features_first_degree['high_school'].replace(np.NaN,'altro')

  #delete support column
  df_all_features_first_degree=df_all_features_first_degree.drop(['other_high_school'], axis=1)
  df_all_features_first_degree.head()

  """### <h5>5.5.6) Controllo finale che non ci siano più valori nulli</h5>"""

  df_all_features_first_degree.isnull().sum()

  """##<h5><b>5.6) Pulizia delle features</b></h5>
  <p> In questa fase abbiamo effettuato la pulizia di tutte le features del nostro dataset. Poichè le nostre caratteristiche sono testuali/categoriali, abbiamo provveduto a:</p>
  <ul>
  <li>Mettere tutte le parole in minuscolo</li>
  <li>Eliminare la punteggiatura</li>
  <li>Eliminare caratteri speciali o spazi bianchi in eccesso</li>
  <li>Stemming: estrazione della radice dalle parole (solo per le carateristiche testuali)</li>
  <li>Eliminazione delle stopwords,ovvero parole inutili ai fini dell'analisi, come le congiunzioni</li>
  <li>Tokenization: trasformazione delle parole in token (solo per le caratteristiche testuali)
  </ul>
  """

  df_clean_features=df_all_features_first_degree.copy()

  categorical_features=['high_school', 'choice_related_studies']
  regex_features=[r'\s\s+'] #delete black space
  for feature in df_clean_features.columns:
    if feature != 'degree_course':
      df_clean_features[feature]=data_cleaning(df_clean_features[feature], regex_features)
      if feature not in categorical_features:
        df_clean_features[feature]=preprocessing_text(df_clean_features,df_clean_features[feature], feature, 'IT')

  #DEBUG: comparison between dirty data and clean data
  for feature in df_clean_features.columns:
    display(pd.merge(df_all_features_first_degree[feature], df_clean_features[feature], right_index = True,left_index = True))
  index_clean_features=df_clean_features.index #need to the final

  """#<h4> 6)FEATURES BUILDING</h4>
  
  ##<h5><b>6.1) Variabile target-> <code>degree_course</code></b></h5>
  <p>Per i corsi di laurea è stato usato un algoritmo di clustering con lo scopo di raggruppare i corsi di laurea  in vari cluster contenenti quelli più simili testualmente tra di loro. Il risultato ottenuto sono stati 120 cluster.</p>
  <p>Innanzitutto abbiamo utilizzato la <b>text-analysis con il modello tf-id</b>, più preciso ed efficace del modello bag of words in quanto tiene conto dell'ordine dei token nella frase.</p>
  <p>Successivamente abbiamo sfruttato l'algoritmo di <b>cosine similarity</b>, che permette di calcolare la distanza tra una parola e l'altra.</p>
  <p>Infine abbiamo applicato l'algoritmo <b>KMeans</b>, per effettuare i vari raggruppamenti</p>
  <p>Per concludere abbiamo soprannominato ogni gruppo con il nome originale (non pulito) della sua prima laurea, al fine di semplificare la visualizzazione dei grafici finali di lettura del modello</p>
  """

  #text analysis
  tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)
  vectorize_matrix=tf_vector(tv, df_clean_features['degree_course'])
  vectorize_matrix

  #cosine similarity
  similarity_matrix = cosine_similarity(vectorize_matrix)
  similarity_df = pd.DataFrame(similarity_matrix)
  similarity_df.head()

  #clustering
  km = KMeans(n_clusters=120, random_state=0)
  km.fit_transform(similarity_df)
  cluster_labels = km.labels_
  cluster_labels_degree_course = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])
  df_clean_features= df_clean_features.set_index(cluster_labels_degree_course.index) #avoids NAN values ​​coming out due to different indexes
  cluster_labels_degree_course.insert(0,'degree_course',df_clean_features['degree_course'], True)
  original_df_whitout_nan_value= original_df_whitout_nan_value.set_index(cluster_labels_degree_course.index) #avoids NAN values ​​coming out due to different indexes
  cluster_labels_degree_course.insert(0,'full_degree_course',original_df_whitout_nan_value['degree_course'], True)
  byCluster=cluster_labels_degree_course.groupby(['ClusterLabel'])

  #DEBUG: print all the cluster
  labels = []
  for cluster, degree in byCluster:
      print(f"All entries for {cluster!r}")
      print("------------------------")
      print(degree, end="\n\n")   #display(degree)
      labels.append(degree['full_degree_course'].iloc[0])
  print(labels)

  #put the cluster result inside the dataset

  df_clean_features=df_clean_features.set_index(cluster_labels_degree_course.index)
  df_clean_features['degree_course']=cluster_labels_degree_course['ClusterLabel']
  #DEBUG
  print('check if add missing values:', df_clean_features['degree_course'].isnull().sum())

  """##<h5><b> 6.2) Caratteristiche categoriali <b></h5>
  <p> Per codificare delle caratteristiche categoriali e binomiali, abbiamo usato l'algoritmo di <code>one hot encoding</code></p>
  """

  encoder_school= LabelEncoder()
  encoder_choice= LabelEncoder()
  for feature in categorical_features:
    print(feature)
    if feature == 'high_school':
      school_labels = encoder_school.fit_transform( df_clean_features[feature])
      df_clean_features[feature]=one_hot_encoding( df_clean_features[feature], feature, school_labels, encoder_school )
    if feature == 'choice_related_studies':
      choice_labels = encoder_choice.fit_transform(df_clean_features[feature])
      df_clean_features[feature]=one_hot_encoding( df_clean_features[feature], feature, choice_labels,encoder_choice )

  school_labels #DEBUG

  """##<h5><b>6.3) Caratteristiche testuali-> text analysis</b></h5>
  <p>Abbiamo deciso di utilizzare due modelli diiferenti per codificare le variabili testuali: il modello <code> Bag of words </code> e il modello <code> TF-IDF</code></p>
  <p> La differenza principale fra essi è la maggiore accuratezza del secondo in quanto considera l'ordine in cui compaiono le parole nelle frasi</p>
  
  ### <h5>6.3.1) Modello bag of words</h5>
  <p>Questo modello è stato utilizzato per tutte quelle features contenenti un elenco testuale, dove perciò era ininfluente l'ordine delle parole ai fini del significato finale. Alcuni campi dove è stato sfruttato questo modello sono:</p>
  <ul>
  <li> Le materie principali</li>
  <li> Le materie preferite</li>
  <li> Gli hobby</li>
  <li> Il lavoro sognato <li>
  </ul>
  """

  #group all the columns in one columns using a list, the we convert the list in string
  df_clean_features['bag_of_words']=df_clean_features[['main_subject', 'hobby' , 'favorite_subject', 'dream_job' ]].values.tolist()
  for index in df_clean_features['bag_of_words'].index:
    item=df_clean_features['bag_of_words'].iloc[index]
    listToStr = ' '.join([str(i) for i in item])
    df_clean_features['bag_of_words'].iloc[index]=listToStr

  #fit the model
  vectorizer_train = CountVectorizer(min_df=0, binary=True)
  df_bow_dict=bag_of_words(vectorizer_train, df_clean_features['bag_of_words'])
  df_bow_dict

  #delete the old columns
  df_clean_features=df_clean_features.drop(['main_subject', 'hobby' , 'favorite_subject', 'dream_job', 'bag_of_words'], axis=1)

  df_clean_features.head() #DEBUG

  #add the new columns
  df_clean_features=df_clean_features.set_index(df_bow_dict.index)
  df_clean_features=pd.concat([df_clean_features,  df_bow_dict], axis=1)

  df_clean_features.head() #DEBUG

  df_clean_features.shape

  df_clean_features.isnull().sum()

  """###<h5>6.3.2) Modello TF-IDF</h5>
  <p>Questo modello è stato usato per codificare le features contenenti frasi, dove perciò l'oridende delle parole avrebbe influito sul significato finale in quanto in base alla disposizione delle parole esso può variare</p>
  <p>Abbiamo perciò utilizzato questo modello per codificare:</p>
  <ul>
  <li> Le aspettative</li>
  <li> Il motivo della scelta</li>
  </ul>
  """

  #group all the columns in one columns using a list, then we convert the list in string
  df_clean_features['tf_if']=df_clean_features[['expectations', 'decision_choice']].values.tolist()
  for index in df_clean_features['tf_if'].index:
    item=df_clean_features['tf_if'].iloc[index]
    listToStr = ' '.join([str(i) for i in item])
    df_clean_features['tf_if'].iloc[index]=listToStr

  tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)
  df_tf_id=tf_vector(tv, df_clean_features['tf_if'])
  df_tf_id

  df_clean_features=df_clean_features.drop(['expectations', 'decision_choice', 'tf_if'], axis=1)

  df_clean_features=df_clean_features.set_index(df_tf_id.index)
  df_clean_features=pd.concat([df_clean_features,  df_tf_id], axis=1)

  df_clean_features.head() #DEBUG

  df_clean_features.shape

  df_clean_features.isnull().sum()

  """#<h4>7) Creazione del dataset"""

  X=df_clean_features.iloc[:, 1:]
  y=df_clean_features['degree_course']

  X #DEBUG

  y

  """##<h5><b>7.1) Bilanciamento delle classi</b></h5>
  <p> Poichè le nostre classi (i corsi di laurea) non sono rappresentate in modo approssimativamente uguale, è stato necessario bilanciare le classi. In questa fase l'obiettivo principale è stato perdere meno dati possibili, il che ha portato a escludere un <code>algoritmo di undersampling</code>, cui risultato sarebbe stata la riduzione del numero dei campioni nelle classi più numerose.  Abbiamo invece optato per un' <code>algoritmo di oversampling</code>, cui scopo è di aumentare il numero dei campioni delle classi meno numerose.</p>
  <p> In particolare abbiamo usato l'algoritmo<code> RandomOverSampler</code> della libreria <i>imblearn</i></p>
  """

  print('number of sample before:', Counter(y)) #DEBUG: see the number of sample before oversampling
  oversample = RandomOverSampler(sampling_strategy='auto')
  X, y= oversample.fit_resample(X, y)
  print('number of sample after:', Counter(y)) #DEBUG: see the number of sample after oversampling

  """###<h5><b>7.2) Suddivisione nel set di test e di addestramento</b></h5>
  <p>In questa fase abbiamo testato gli algoritmi con 4 test set e train set differenti,con lo scopo di verificare in quale caso la predizione fosse migliore e più accurata</p>
  """

  X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(X, y, test_size=0.20, random_state=42)
  X_train_33, X_test_33, y_train_33, y_test_33 = train_test_split(X, y, test_size=0.33, random_state=42)
  X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(X, y, test_size=0.50, random_state=42)
  X_train_66, X_test_66, y_train_66, y_test_66 = train_test_split(X, y, test_size=0.66, random_state=42)
  X_train_list=[X_train_20, X_train_33, X_train_50, X_train_66]
  y_train_list=[y_train_20, y_train_33, y_train_50, y_train_66]
  X_test_list=[X_test_20, X_test_33, X_test_50, X_test_66]
  y_test_list=[y_test_20, y_test_33, y_test_50, y_test_66]
  keys_list=['test_20%', 'test_33%', 'test_50%', 'test_66%']

  """#<h4>8) PROCESSI DI ADDESTRAMENTO E TECNICHE/ALGORITMI UTILIZZATI</h4> 
  <p> Il dataset è stato prima suddiviso in 4 train/test set differenti e successivamente ogni set è stato addestrato con 4 algoritmi di <code>ensemble learning</code> differenti.<br> Infine è stato creato un algoritmo che, prendendo in input i risultati di ciascuna predizione confrontasse i risultati per ottenere in output l'algoritmo migliore e il set migliore su cui addestrare quest'algoritmo</p>
  
  ##<h5><b> 8.1) Ensemble Voting</b></h5>
  
  <p>Un voting ensemble (anche chiamato “majority voting ensemble“) è un modello ensemble di machine learning che combina le predizioni di più modelli. Può essere utilizzato per la classificazione o la regressione. Nel caso della regressione, ciò comporta il calcolo della media delle previsioni dai modelli. Nel caso della classificazione vengono sommati i pronostici per ogni etichetta e si pronostica l'etichetta con il voto di maggioranza.
  <br>
  <b>Regression Voting Ensemble: </b> le previsioni sono la media dei modelli che contribuiscono.<br>
  <b>Classification Voting Ensemble:</b> le previsioni sono il voto di maggioranza dei modelli che contribuiscono.
  
  Vi sono due possibili approcci alla previsione del voto di maggioranza per la classificazione quali hard voting e soft voting.
  
  <b>L'hard voting</b> implica la somma delle previsioni per ogni etichetta di classe e la previsione dell'etichetta di classe con il maggior numero di voti. Il <b>soft voting</b> implica la somma delle probabilità previste per ogni etichetta di classe e la previsione dell'etichetta di classe con la probabilità più alta.
  
  </p>
  
  <p>Come stimatori di base abbiamo utilizzato 2 algoritmi di classificazione che si basano sul <code>l'algoritmo di Nayve Bayes</code> e 2 algoritmi che si basano sui <code>Decision tree</code>, in quanto questo genere di algoritmi sono migliori per effettuare le predizioni date delle features testuali.
  """

  # initializing all the model objects with default parameters

  model_1 = MultinomialNB()
  model_2 = DecisionTreeClassifier()
  model_3 = SVC()
  estimators_list=[('NB', model_1), ('DT', model_2), ('SVM', model_3)]

  # Making the final model using voting classifier
  voting_model = VotingClassifier(
      estimators=estimators_list, voting='hard')

  voting_predict={}
  voting_score={}
  voting_report={}

  for i in range(0,4):
    voting_predict[keys_list[i]]=fit_dataset(voting_model, X_train_list[i], y_train_list[i], X_test_list[i])
    voting_score[keys_list[i]]=score_dataset(voting_model, X_train_list[i], y_train_list[i], X_test_list[i], y_test_list[i])
    voting_score[keys_list[i]]['wrong_class']=wrong_classification(X_test_list[i], voting_predict[keys_list[i]], y_test_list[i])
    voting_report[keys_list[i]]=classification_report(y_test_list[i], voting_predict[keys_list[i]], output_dict=True)
    voting_score[keys_list[i]]['accuracy']=voting_report[keys_list[i]]['accuracy']

  """##<h5><b>8.2) Ensemble Bagging </b></h5>
  <p>Il bagging, metodo di ensemble learning parallelo (acronimo di Bootstrap Aggregating), è un modo per ridurre la varianza del modello di previsione generando dati aggiuntivi nella fase di addestramento. Questo è prodotto da un campionamento casuale con sostituzione dal set originale. Campionando con sostituzione, alcune osservazioni possono essere ripetute in ogni nuovo set di dati di addestramento. 
  
  Nel caso del Bagging, ogni elemento ha la stessa probabilità di apparire in un nuovo set di dati. Aumentando la dimensione del training set, la forza predittiva del modello non può essere migliorata. Riduce la varianza e sintonizza la previsione su un risultato atteso.</p>
  <p>Questi multiset di dati vengono utilizzati per addestrare più modelli. Di conseguenza, ci ritroviamo con un insieme di modelli diversi. Viene utilizzata la media di tutte le previsioni di diversi modelli. Questo lo rende più robusto di un singolo modello. La previsione può essere la media di tutte le previsioni fornite dai diversi modelli in caso di regressione. In caso di classificazione si tiene conto del voto di maggioranza.</p>
  """

  # Making the final model using bagging classifier
  bagging_model = BaggingClassifier(
      base_estimator=None,n_estimators=10,random_state=0)

  bagging_predict={}
  bagging_score={}
  bagging_report={}

  for i in range(0,4):
    bagging_predict[keys_list[i]]=fit_dataset(bagging_model, X_train_list[i], y_train_list[i], X_test_list[i])
    bagging_score[keys_list[i]]=score_dataset(bagging_model, X_train_list[i], y_train_list[i], X_test_list[i], y_test_list[i])
    bagging_score[keys_list[i]]['wrong_class']=wrong_classification(X_test_list[i], bagging_predict[keys_list[i]], y_test_list[i])
    bagging_report[keys_list[i]]=classification_report(y_test_list[i], bagging_predict[keys_list[i]], output_dict=True)
    bagging_score[keys_list[i]]['accuracy']=bagging_report[keys_list[i]]['accuracy']

  """##<h5><b>8.3) Ensemble Boosting</b></h5>
  <p>Il boosting è un metodo di ensemble learning sequenziale che in generale riduce l'errore di bias e costruisce modelli predittivi forti. Il termine "Boosting" si riferisce a una famiglia di algoritmi che converte un learner debole in un learner forte.
  
  Il Boosting utilizza più learners. I campioni di dati sono ponderati perciò alcuni di essi possono partecipare più spesso ai nuovi set.
  
  In ogni iterazione, vengono identificati i dati che sono stati previsti erroneamente e i loro pesi vengono aumentati in modo che il successivo learner successivo presti maggiore attenzione ad essi. </p>
  """

  # Making the final model using boosting classifier
  boosting_model = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=1, random_state=0)

  boosting_predict={}
  boosting_score={}
  boosting_report={}

  for i in range(0,4):
    boosting_predict[keys_list[i]]=fit_dataset(boosting_model, X_train_list[i], y_train_list[i], X_test_list[i])
    boosting_score[keys_list[i]]=score_dataset(boosting_model, X_train_list[i], y_train_list[i], X_test_list[i], y_test_list[i])
    boosting_score[keys_list[i]]['wrong_class']=wrong_classification(X_test_list[i], boosting_predict[keys_list[i]], y_test_list[i])
    boosting_report[keys_list[i]]=classification_report(y_test_list[i], boosting_predict[keys_list[i]], output_dict=True)
    boosting_score[keys_list[i]]['accuracy']=boosting_report[keys_list[i]]['accuracy']

  """##<h5>8.4) Ensemble Stacking</h5>
  <p>Lo stacking, noto anche come Stacked Generalization, è una tecnica di ensemble learning che combina più classificazioni o modelli di regressione tramite un meta-classificatore o un meta-regressore. I modelli di livello base vengono addestrati su un set di addestramento completo, quindi il meta-modello viene addestrato sulle funzionalità che sono output del modello di livello base. Il livello base è spesso costituito da diversi algoritmi di apprendimento,perciò gli stacking ensemble sono spesso eterogenei.</p>
  """

  # Making the final model using stacking classifier
  stacking_model = StackingClassifier(
      estimators=estimators_list, final_estimator=LogisticRegression())

  stacking_predict={}
  stacking_score={}
  stacking_report={}

  for i in range(0,4):
    stacking_predict[keys_list[i]]=fit_dataset(stacking_model, X_train_list[i], y_train_list[i], X_test_list[i])
    stacking_score[keys_list[i]]=score_dataset(stacking_model, X_train_list[i], y_train_list[i], X_test_list[i], y_test_list[i])
    stacking_score[keys_list[i]]['wrong_class']=wrong_classification(X_test_list[i], stacking_predict[keys_list[i]], y_test_list[i])
    stacking_report[keys_list[i]]=classification_report(y_test_list[i], stacking_predict[keys_list[i]], output_dict=True)
    stacking_score[keys_list[i]]['accuracy']=stacking_report[keys_list[i]]['accuracy']

  """#<h4>9) COMPARAZIONE ED ESPORTAZIONE DEI RISULTATI OTTENUTI</h4>
  
  ##<h5><b>9.1)Visualizzazione dei risultati</b></h5>

  ###<h5> 9.1.2)Tabelle dei risultati</h5>
  """
  # TODO
  print('------------Ensemble voting-------------------')
  voting_score_df=pd.DataFrame(voting_score)
  display(voting_score_df)
  print('------------Ensemble bagging-------------------')
  bagging_score_df=pd.DataFrame(bagging_score)
  display(bagging_score_df)
  print('------------Ensemble boosting-------------------')
  boosting_score_df=pd.DataFrame(boosting_score)
  display(boosting_score_df)
  print('------------Ensemble stacking-------------------')
  stacking_score_df=pd.DataFrame(stacking_score)
  display(stacking_score_df)

  """###<h5>9.1.2) Grafici dei risultati</h5>"""

  plt.figure(figsize = (15,8))
  plt.plot(voting_score_df.columns,list(voting_score_df.loc['accuracy']))
  plt.plot(bagging_score_df.columns,list(bagging_score_df.loc['accuracy']))
  plt.plot(boosting_score_df.columns,list(boosting_score_df.loc['accuracy']))
  plt.plot(stacking_score_df.columns,list(stacking_score_df.loc['accuracy']))
  plt.legend(['voting', 'bagging', 'boosting','stacking'])
  plt.show()

  plt.figure(figsize = (8,8))
  plt.plot(voting_score_df.columns,list(voting_score_df.loc['accuracy']), 's')
  plt.plot(stacking_score_df.columns,list(stacking_score_df.loc['accuracy']),'o')
  plt.plot(bagging_score_df.columns,list(bagging_score_df.loc['accuracy']), '*')
  plt.plot(boosting_score_df.columns,list(boosting_score_df.loc['accuracy']), '^')
  plt.legend(['voting',  'stacking', 'bagging', 'boosting'])
  plt.axis(ymax=1)
  plt.xlabel("Test set")
  plt.show()

  """##<h5><b>9.2) Scelta del modello migliore</b></h5>
  <p>L'algoritmo seguente seleziona il modello migliore tra tutte le predizione effettuate in tutti i set,indica perciò quale set dobbiamo utilizzare e con quale algoritmo utilizzarlo</p>
  """

  #creation of the accurancy dataframe
  list_score=[voting_score_df, bagging_score_df, boosting_score_df, stacking_score_df]
  name_list=['voting', 'bagging', 'boosting', 'stacking']
  accurancy_df=pd.DataFrame()
  for i in range(0,4):
    df=list_score[i].transpose()
    label=name_list[i]
    accurancy_df.insert(0, label, df['accuracy'])
  accurancy_df

  df2=accurancy_df.transpose()
  dict2={}
  l=[]
  for key in keys_list:
    dict2[key]=df2[df2[key]==df2[key].max()][key].to_dict() #dictionary containing for each test set the algorithm with the highest accuracy
    l.append(list(dict2[key].values())) #support list
  #find the test set with the best accuracy
  max_model=max(l)
  for test, sub_dict in  dict2.items():
    if(list(sub_dict.values())==max_model): #get the best algorithm and test set back
      for model, value in sub_dict.items():
        model_name=model
        test_name=test
  print('Il miglior modello è', model_name, ', utilizzando un', test_name)

  #creation of the best model
  error=0
  if model_name=='voting':
    clf=voting_model
    report=voting_report[test_name]
    predictions=voting_predict[test_name]
    print('voting') #debug output
  elif model_name=='bagging':
    clf=bagging_model
    report=bagging_report[test_name]
    predictions=bagging_predict[test_name]
    print('bagging')
  elif model_name=='boosting':
    clf=boosting_model
    report=boosting_report[test_name]
    predictions=boosting_predict[test_name]
    print('boosting')
  elif model_name=='stacking':
    clf=stacking_model
    report=stacking_report[test_name]
    predictions=stacking_predict[test_name]
    print('stacking')
  else:
    print('Errore nella scelta del modello')
    error=1

  #I get the correct index for the lists
  def get_index_from_test(test_name, name_list):
    c=0
    for key in name_list:
      if key==test_name:
        return c
      c+=1
  index=get_index_from_test(test_name, keys_list)
  index

  """##<h5><b>9.3) Valutazione del modello</b></h5>
  <p> Per valutare il modello è stata usata <code> la matrice di confusione</code> assieme alle relative metriche.
  
  ###<h5>9.3.1) Report di classificazione</h5>
  <p> Il report di classificazione indica:</p>
  <ul>
  <li>La precisione: nota anche come valore predittivo positivo, viene definito come il numero di previsioni effettuate che sono effettivamente corrette o rilevanti tra tutte le previsioni basate sulla classe positiva </li>
  <li>La recall: nota anche come sensibilità, è una misura di un modello per identificare la percentuale di punti dati rilevanti. È definita come il numero di istanze della classe positiva che sono state correttamente previste. Questo è anche noto come tasso di successo, copertura o sensibilità.
  <li> La funzione di f1-score: è una metrica che è la media armonica di precisione e richiamo e ci aiuta a ottimizzare un classificatore per una precisione bilanciata e prestazioni di richiam</li>
  <li> La support:numero di occorrenze della classe data nel dataset</li>
  </ul>
  """

  if error!=1:
    index_report=[]
    report_precision=[]
    report_recall=[]
    report_f1=[]
    report_support=[]
    for i in range(0,120):
      item=str(i)
      report_precision.append(report[item]['precision'])
      report_recall.append(report[item]['recall'])
      report_f1.append(report[item]['f1-score'])
      report_support.append(report[item]['support'])
      index_report.append(i)




    report_df=pd.DataFrame(report_precision, index=labels, columns=['precision'])
    report_df.insert(len(report_df.columns), 'recall', report_recall)
    report_df.insert(len(report_df.columns), 'f1-score', report_f1)
    report_df.insert(len(report_df.columns), 'support', report_support)
    start=0
    end=60
    while end<=120:
      display(report_df[start:end])
      print("")
      start=end
      end+=60
  else:
    print("Errore: non sono riuscito a calcolare l'algoritmo migliore")

  """###<h5>9.3.2) Grafico di precisione dei risultati</h5>"""

  if error!=1:
    start=0
    end=30
    while end <= 120:
      title_str=f'Metriche di precisione da {labels[start]} a {labels[end-1]}'
      plt.figure(figsize = (10,10))
      sns.set(font_scale=1)
      sns.pointplot( x=report_precision[start:end], y=labels[start:end], markers='s', linestyles='')
      plt.title(title_str)
      plt.xlabel("Valori")
      plt.ylabel("Corso di laurea")
      start=end
      end+=30
  else:
    print("Errore: non sono riuscito a calcolare l'algoritmo migliore")

  """###<h5>9.3.1)Matrice di confusione</h5>
  <p> La matrice di confusione è stata splittata in 4 grafici per migliorarne la leggibilità</p>
  """

  if error!=1:
    cm = confusion_matrix(y_test_list[index], clf.predict(X_test_list[index]))
  else:
    print("Errore: non sono riuscito a calcolare l'algoritmo migliore")

  if error!=1:
    start=0
    end=30
    while end <= 120:
      df_cm = pd.DataFrame(cm[start:end, 0:120])
      title_str=f'Matrice di confusione da {labels[start]} a {labels[end-1]}'
      plt.figure(figsize = (60,20))
      sns.set(font_scale=2.5)
      sns.heatmap(df_cm, annot=True, fmt="d", annot_kws={"size": 20}, xticklabels=labels[0:120], yticklabels=labels[start:end])# font size
      plt.title(title_str)
      plt.show()
      start=end
      end+=30

  else:
    print("Errore: non sono riuscito a calcolare l'algoritmo migliore")

  """#PARTE FINALE DOMANDE A STUDENTE"""

  #@title scelta università { run: "auto", vertical-output: true }
  #@markdown Che scuola superiore hai fatto?
  scuola_superiore = "Liceo Scientifico" #@param ["Liceo Scientifico", "Istituto tecnico Industriale", "Liceo Classico", "Istituto tecnico economico", "Liceo delle scienze umane"]
  #@markdown Principalmente quali sono le materie che hai studiato?
  materie_studiate = "Matematica, fisica, informatica, scienze" #@param {type:"string"}
  #@markdown Quali sono le tue materie preferite?
  materie_preferite = "scienze, chimica" #@param {type:"string"}
  #@markdown Quali sono i tuoi hobby?
  hobby = "sport, musica, guardare film" #@param {type:"string"}
  #@markdown Cosa vorresti fare da grande?
  lavoro_sognato = "medico" #@param {type:"string"}
  #@markdown Cosa ti aspetti dall'università?
  aspettative_universita = "Mi aspetto di imparare molto e essere preparata per il mondo del lavoro" #@param {type:"string"}
  #@markdown Perchè vorresti affrontare un percorso universitario?
  motivo_scelta_universita = "Voglio continuare a studiare per fare il lavoro dei mie sogni, mi piace soprattutto l'ambito scientifico e medico" #@param {type:"string"}
  #@markdown Vorresti continuare i tuoi studi precedenti?
  continuare_studi_precedenti = "SI" #@param ["SI", "NO"]


  df_answer=pd.DataFrame([scuola_superiore, materie_studiate, materie_preferite, hobby, lavoro_sognato, aspettative_universita, motivo_scelta_universita, continuare_studi_precedenti]).transpose()
  df_answer.columns=['scuola_superiore', 'materie_studiate', 'materie_preferite', 'hobby', 'lavoro_sognato', 'aspettative_universita', 'motivo_scelta_universita', 'continuare_studi_precedenti']
  df_answer

  #gestisci il caso in cui ci sia un valore nullo

  df_answer_clean=df_answer.copy()

  categorical_features_answer=['scuola_superiore', 'continuare_studi_precedenti']
  regex_features=[r'\s\s+'] #delete black space
  for feature in df_answer_clean.columns:
    df_answer_clean[feature]=data_cleaning(df_answer_clean[feature], regex_features)
    if feature not in categorical_features_answer:
      df_answer_clean[feature]=preprocessing_text(df_answer_clean,df_answer_clean[feature], feature, 'IT')

  df_answer_clean

  """##One hot encoding"""

  number=0
  for class_ in encoder_choice.classes_:
    for value in df_answer_clean['continuare_studi_precedenti']:
      if class_== value:
        print(number)
        df_answer_clean['continuare_studi_precedenti']=number
      else:
        number+=1

  number=0
  for class_ in encoder_school.classes_:
    for value in df_answer_clean['scuola_superiore']:
      if class_== value:
        print(number)
        df_answer_clean['scuola_superiore']=number
      else:
        number+=1

  df_answer_clean

  """###Bag of words"""


  #group all the columns in one columns using a list, the we convert the list in string
  df_answer_clean['bag_of_words']=df_answer_clean[['materie_studiate', 'hobby' , 'materie_preferite', 'lavoro_sognato' ]].values.tolist()
  for index in df_answer_clean['bag_of_words'].index:
    item=df_answer_clean['bag_of_words'].iloc[index]
    listToStr = ' '.join([str(i) for i in item])
    df_answer_clean['bag_of_words'].iloc[index]=listToStr

  msg_array = vectorizer_train.transform(df_answer_clean['bag_of_words']).toarray()
  msg_array

  vocab = vectorizer_train.get_feature_names()
  df_bow_answer=pd.DataFrame(msg_array, columns=vocab)
  #CODICE DI TEST
  #for key in df_bow_answer:
    #if df_bow_answer[key].values==1:
      #print(key)

  #delete the old columns
  df_answer_clean=df_answer_clean.drop(['materie_studiate', 'hobby' , 'materie_preferite', 'lavoro_sognato', 'bag_of_words' ], axis=1)

  #add the new columns
  df_answer_clean=df_answer_clean.set_index(df_bow_answer.index)
  df_answer_clean=pd.concat([df_answer_clean,  df_bow_answer], axis=1)
  df_answer_clean


  """###modello TF-IDF"""

  #group all the columns in one columns using a list, then we convert the list in string
  df_answer_clean['tf_if']=df_answer_clean[['aspettative_universita', 'motivo_scelta_universita']].values.tolist()
  for index in df_answer_clean['tf_if'].index:
    item=df_answer_clean['tf_if'].iloc[index]
    listToStr = ' '.join([str(i) for i in item])
    df_answer_clean['tf_if'].iloc[index]=listToStr

  msg_array_tf=tv.transform(df_answer_clean['tf_if']).toarray()
  msg_array_tf

  vocab = tv.get_feature_names()
  df_tf_answer=pd.DataFrame(msg_array_tf, columns=vocab)

  #delete the old columns
  df_answer_clean=df_answer_clean.drop(['aspettative_universita', 'motivo_scelta_universita', 'tf_if'], axis=1)

  df_answer_clean=df_answer_clean.set_index(df_tf_answer.index)
  df_answer_clean=pd.concat([df_answer_clean,  df_tf_answer], axis=1)
  df_answer_clean

  df_answer_clean.values.shape

  # TODO
  prediction_uni=clf.predict(df_answer_clean.values)
  for i in prediction_uni:
    uni=i
  print(uni)

  """#PARTE FINALE UNIVERSITA'"""

  #indexs=zip(list(df_clean_features.index), index_clean_features)
  ##df_index=pd.DataFrame(data=indexs, columns=['after_cleaning', 'before_cleaning'])
  #df_index

  #a=list(df_clean_features[df_clean_features['degree_course']==11].index)
  #lst=[]
  #for i in a:
   # out=df_index[df_index['after_cleaning']==i]['before_cleaning'].values
   # lst.append(out)
   # print(out)
  #print(lst)

  all_features_final=df_students_first_degree
  all_features_final=all_features_final.append(df_graduate_first_degree, ignore_index=True, verify_integrity=True)
  all_features_final=all_features_final.iloc[list(index_clean_features)]

  final_features=['degree_course','university','other_uni',  'stars', 'hometown', 'study_town', 'didactic_quality', 'teaching_quality', 'exams_difficulties', 'subjects_difficulties', 'students_relationship', 'laboratories']
  df_final=all_features_final[final_features]
  df_final.shape

  df_final = df_final[df_final['university'].notna()]
  df_final.shape

  df_stars_mean=df_final[df_final['stars'].notna()]
  series_review_uni=df_stars_mean.groupby('university')['stars'].mean()
  series_review_uni

  #recupero lauree dato cluster
  df2=cluster_labels_degree_course[cluster_labels_degree_course['ClusterLabel']==uni]['full_degree_course']

  #recupero uni data laurea
  degrees=[]

  df=pd.DataFrame()
  for degree in df2.unique():
    degrees.append(degree)

    df= df.append(list(df_final[df_final['degree_course']==degree][['university',  'didactic_quality', 'teaching_quality', 'exams_difficulties', 'subjects_difficulties', 'students_relationship', 'laboratories']].values))
  df.columns=['Università',  'Qualità didattica', 'Qualità insegnanti', 'Difficoltà esami', 'Difficoltà materie', 'Integrazione studentesca', 'Laboratori']
  print(degrees)
  print(df)
  print(np.unique(df['Università']))

  valutation_uni=df.groupby('Università')[[ 'Qualità didattica', 'Qualità insegnanti', 'Difficoltà esami', 'Difficoltà materie', 'Integrazione studentesca', 'Laboratori']].mean()

  valutation_uni.insert(0, 'Università', valutation_uni.index)

  valutation_uni

  df1 = pd.melt(valutation_uni, id_vars='Università', var_name="type", value_name="valutation")
  df1

  title_label="Valutazione per "+ degrees[0]

  plt.figure()
  sns.barplot(valutation_uni['Qualità didattica'], valutation_uni.index)
  plt.grid(axis='y')
  plt.xlim((0,10))
  plt.title(title_label)
  plt.figtext(-1, 1, "Legenda valutazioni qualità didattica: \n0 = Qualità didattica scadente \n10 = Ottima qualità didattica")
  plt.show()

  plt.figure()
  sns.barplot(valutation_uni['Qualità insegnanti'], valutation_uni.index)
  plt.grid(axis='y')
  plt.xlim((0,10))
  plt.title(title_label)
  plt.figtext(-1, 1, "Legenda valutazioni qualità docenti: \n0 = Bassa preparazione dei docenti \n10 = Ottima preparazione dei docenti")
  plt.show()

  plt.figure()
  sns.barplot( valutation_uni['Difficoltà esami'], valutation_uni.index)
  plt.grid(axis='y')
  plt.xlim((0,10))
  plt.title(title_label)
  plt.figtext(-1, 1, "Legenda valutazioni difficoltà esami: \n0 = Esami facili \n10 = Esami difficili")
  plt.show()

  plt.figure()
  sns.barplot(valutation_uni['Difficoltà materie'], valutation_uni.index)
  plt.grid(axis='y')
  plt.xlim((0,10))
  plt.title(title_label)
  plt.figtext(-1, 1, "Legenda valutazioni difficoltà materie: \n0 = Materie facili da studiare \n10 = materie difficili da studiare")
  plt.show()

  plt.figure()
  sns.barplot(valutation_uni['Integrazione studentesca'], valutation_uni.index)
  plt.grid(axis='y')
  plt.xlim((0,10))
  plt.title(title_label)
  plt.figtext(-1, 1, "Legenda valutazioni integrazione tra studenti: \n0 = Poca collaborazione \n10 = Molta collaborazione")
  plt.show()

  plt.figure()
  sns.barplot(valutation_uni['Laboratori'], valutation_uni.index)
  plt.grid(axis='y')
  plt.xlim((0,10))
  plt.title(title_label)
  plt.figtext(-1, 1, "Legenda valutazioni laboratori: \n0 = Nessun attività pratica \n10 = Molte attività pratiche")
  plt.show()

  df_review_uni=series_review_uni.to_frame()
  df_review_uni.insert(0, 'university', df_review_uni.index)
  degree_review=pd.DataFrame()
  for key in valutation_uni.index:
    degree_review=degree_review.append(list(df_review_uni[df_review_uni['university']==key][['university', 'stars']].values))
  degree_review.columns=['università', 'stelle']
  degree_review

  out_images=degree_review
  sns.barplot(data=out_images, x='stelle', y='università')
  plt.grid(axis='y')
  plt.xlim((0,5))
  plt.title("Recensioni per " +degree )
  plt.show()